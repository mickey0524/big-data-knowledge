# big-data-knowledge
ğŸ“–å¤§æ•°æ®ç›¸å…³çŸ¥è¯†é›†é”¦

* [hdfs](#hdfs)
* [yarn](#yarn)
* [hive](#hive)
* [mapreduce](#mapreduce)
* [spark](#spark)
* [hbase](#hbase)
* [zookeeper](#zk)
* [kafka](#kafka)

<h3 id="hdfs">hdfs</h3>

* HDFSç®€ä»‹

	HDFSæ˜¯Hadoop Distributed File Systemçš„ç®€å†™
	
	* HDFSå…·æœ‰é«˜å®¹é”™æ€§å’Œé«˜ååæ€§çš„ç‰¹ç‚¹
	* HDFSç›®å‰æ˜¯ append onlyï¼Œæš‚æ—¶ä¸æ”¯æŒéšæœº write çš„æ“ä½œ
	* HDFSé€‚åˆç”¨äºå­˜å‚¨ä»¥åŠæ‰¹é‡æ“ä½œå¤§è§„æ¨¡çš„æ•°æ®é›†(PBçº§åˆ«)
	* ä¸é€‚åˆå®æ—¶è®¿é—®ï¼Œå…·æœ‰é«˜å»¶è¿Ÿæ€§ï¼Œä¾‹å¦‚æ–°å»ºäº†ä¸€å¼ hiveè¡¨ï¼Œéœ€è¦è¿‡ä¸€ä¼šæ‰èƒ½çœ‹åˆ°
	* [Hadoop HDFS æ•™ç¨‹ï¼ˆä¸€ï¼‰ä»‹ç»](https://www.jianshu.com/p/8969eb90a59d)
	* [Hadoop HDFSï¼ˆäºŒï¼‰ç»“æ„è§£æå’Œåè¯è§£é‡Š](https://www.jianshu.com/p/86a70ac1f5f9)

* HDFSå­˜åœ¨ä¸€ä¸ªå•ç‚¹é—®é¢˜ï¼Œå³å…¨Hadoopç³»ç»Ÿåªæœ‰ä¸€ä¸ªNameNodeï¼Œå¦‚æœNameNodeæŒ‚äº†æ€ä¹ˆåŠ

    * å°†hadoopå…ƒæ•°æ®å†™å…¥åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿçš„åŒæ—¶ï¼Œå†å®æ—¶åŒæ­¥åˆ°ä¸€ä¸ªè¿œç¨‹æŒ‚è½½çš„ç½‘ç»œæ–‡ä»¶ç³»ç»Ÿ
    * è¿è¡Œä¸€ä¸ªsecondaryNameNode
        * å…ƒæ•°æ®æŒä¹…åŒ–åˆ°ç£ç›˜ï¼Œåœ¨fsimageä¸­å­˜æ”¾å…ƒä¿¡æ¯ï¼Œåœ¨editsä¸­å­˜æ”¾å¯¹å…ƒä¿¡æ¯çš„æ“ä½œçš„æ–‡ä»¶
        * å®šæ—¶åˆ°NameNodeä¸­å»è·å–edit logsï¼Œå¹¶æ›´æ–°åˆ°fsimage
        * ä¸€æ—¦å®ƒæœ‰äº†æ–°çš„fsimageæ–‡ä»¶ï¼Œå®ƒå°†å…¶æ‹·è´å›NameNodeä¸­
        * NameNodeåœ¨ä¸‹æ¬¡é‡å¯æ—¶ä¼šä½¿ç”¨è¿™ä¸ªæ–°çš„fsimageæ–‡ä»¶ï¼Œä»è€Œå‡å°‘é‡å¯çš„æ—¶é—´

* HDFSä¸­çš„å—ä¸ºä»€ä¹ˆè¿™ä¹ˆå¤§ï¼Ÿ

    HDFSçš„å—æ¯”ç£ç›˜çš„å—å¤§ï¼Œå…¶ç›®çš„æ˜¯ä¸ºäº†æœ€å°åŒ–å¯»å€å¼€é”€ã€‚å¦‚æœå—è¶³å¤Ÿå¤§ï¼Œä»ç£ç›˜ä¼ è¾“æ•°æ®çš„æ—¶é—´ä¼šæ˜æ˜¾å¤§äºå®šä½è¿™ä¸ªå—å¼€å§‹ä½ç½®æ‰€éœ€çš„æ—¶é—´ã€‚å› è€Œï¼Œä¼ è¾“ä¸€ä¸ªç”±å¤šä¸ªå—ç»„æˆçš„å¤§æ–‡ä»¶çš„æ—¶é—´å–å†³äºç£ç›˜ä¼ è¾“é€Ÿç‡

* HDFSçš„è¯»æµç¨‹å’Œå†™æµç¨‹

    è¯»è¿‡ç¨‹

    ![read-hdfs](./imgs/read-hdfs.jpg)

    å†™è¿‡ç¨‹

    ![write-hdfs](./imgs/write-hdfs.jpg)

* HDFSé€šè¿‡CRCæ ¡éªŒæ¥ä¿è¯æ•°æ®çš„æ­£ç¡®æ€§
 
* proquetåˆ—å¼å­˜å‚¨

    * [æ·±å…¥åˆ†æParquetåˆ—å¼å­˜å‚¨æ ¼å¼](http://www.infoq.com/cn/articles/in-depth-analysis-of-parquet-column-storage-format)
    * [Dremel made simple with Parquet](https://blog.twitter.com/engineering/en_us/a/2013/dremel-made-simple-with-parquet.html)
    * [Dremel: Interactive Analysis of Web-Scale Datasets](http://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/36632.pdf)

<h3 id="yarn">yarn</h3>

* yarnç®€ä»‹

	yarnæ˜¯hadoopå†…éƒ¨çš„èµ„æºç®¡ç†ç³»ç»Ÿ
	
	* èµ„æºç®¡ç†(10kçš„æœºå™¨æ•°)
		* CPUï¼ŒMemory...
		* èµ„æºåˆ©ç”¨ & å…±äº«
	* è°ƒåº¦/ç›‘æ§åˆ†å¸ƒå¼jobs
	* ç»Ÿä¸€çš„æ¥å£ç®¡ç†
		* MapReduce
		* Spark
		* Flink

* YARNæ˜¯hadoopçš„é›†ç¾¤èµ„æºç®¡ç†ç³»ç»Ÿï¼ŒYARNè¢«å¼•å…¥Hadoop 2ï¼Œæœ€åˆæ˜¯ä¸ºäº†æ”¹å–„MapReduceçš„å®ç°ï¼Œä½†å®ƒå…·æœ‰è¶³å¤Ÿçš„é€šç”¨æ€§ï¼Œä¹Ÿå¯ä»¥ç”¨äºå…¶ä»–çš„åˆ†å¸ƒå¼è®¡ç®—æ¨¡å¼ï¼Œä¾‹å¦‚Sparkï¼Œé‚£ä¹ˆMapReduce1å’ŒYARNçš„åŒºåˆ«æ˜¯å•¥å‘¢ï¼Ÿ

    MapReduce1ä¸­ï¼Œæœ‰ä¸¤ç±»å®ˆæŠ¤è¿›ç¨‹æ§åˆ¶è€…ä½œä¸šçš„æ‰§è¡Œè¿‡ç¨‹ï¼šä¸€ä¸ª`jobtracker`åŠä¸€ä¸ªæˆ–å¤šä¸ª`tasktracker`ã€‚jobtrackeré€šè¿‡è°ƒåº¦tasktrackerä¸Šè¿è¡Œçš„ä»»åŠ¡æ¥åè°ƒæ‰€æœ‰è¿è¡Œåœ¨ç³»ç»Ÿä¸Šçš„ä½œä¸šã€‚tasktrackeråœ¨è¿è¡Œä»»åŠ¡çš„åŒæ—¶å°†è¿è¡Œè¿›åº¦æŠ¥å‘Šå‘é€ç»™jobtrackerï¼Œjobtrackerç”±æ­¤è®°å½•æ¯é¡¹ä½œä¸šä»»åŠ¡çš„æ•´ä½“è¿›åº¦æƒ…å†µã€‚å¦‚æœå…¶ä¸­ä¸€ä¸ªä»»åŠ¡å¤±è´¥ï¼Œjobtrackerå¯ä»¥åœ¨å¦ä¸€ä¸ªtasktrackerèŠ‚ç‚¹ä¸Šé‡æ–°è°ƒåº¦è¯¥ä»»åŠ¡ã€‚

    MapReduce1ä¸­ï¼ŒjobtrackeråŒæ—¶è´Ÿè´£ä½œä¸šè°ƒåº¦(å°†ä»»åŠ¡ä¸tasktrackeråŒ¹é…)å’Œä»»åŠ¡è¿›åº¦ç›‘æ§(è·Ÿè¸ªä»»åŠ¡ã€é‡å¯å¤±è´¥æˆ–è¿Ÿç¼“çš„ä»»åŠ¡ï¼›è®°å½•ä»»åŠ¡æµæ°´ï¼Œå¦‚ç»´æŠ¤è®¡æ•°å™¨çš„è®¡æ•°)ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒYARNä¸­ï¼Œè¿™äº›èŒè´£æ˜¯ç”±ä¸åŒçš„å®ä½“æ‹…è´Ÿçš„ï¼šèµ„æºç®¡ç†å™¨å’Œapplication master(æ¯ä¸ª MapReduce ä½œä¸šä¸€ä¸ª)ã€‚jobtrackerä¹Ÿè´Ÿè´£å­˜å‚¨å·²å®Œæˆä½œä¸šçš„ä½œä¸šå†å²ã€‚åœ¨YARNä¸­ï¼Œä¸ä¹‹ç­‰ä»·çš„è§’è‰²æ˜¯æ—¶é—´è½´æœåŠ¡å™¨ï¼Œå®ƒä¸»è¦ç”¨äºå­˜å‚¨åº”ç”¨å†å²ã€‚

    YARNä¸­ä¸tasktrackerç­‰ä»·çš„è§’è‰²æ˜¯èŠ‚ç‚¹ç®¡ç†å™¨ã€‚
    
    | MapReduce1 | YARN |
    | ---------- | ---- |
    | Jobtracker | èµ„æºç®¡ç†å™¨ã€application masterã€æ—¶é—´è½´æœåŠ¡å™¨|
    | Tasktracker| èŠ‚ç‚¹ç®¡ç†å™¨ |
    | Slot | å®¹å™¨ |
    
* YARNä¸­å­˜åœ¨ä¸‰ç§è°ƒåº¦æ–¹æ³•

	* FIFO
	* å®¹å™¨è°ƒåº¦å™¨
	* å…¬å¹³è°ƒåº¦å™¨

<h3 id="hive">hive</h3>

* æ•°æ®ä»“åº“(DW/Data Warehouse)åˆ†å±‚åŸåˆ™(æ¯å®¶å…¬å¸éƒ½æœ‰è‡ªå·±çš„è§„èŒƒ)

	* dimï¼šç»´åº¦å±‚ï¼Œä¸€èˆ¬ç”¨äºå­˜å‚¨å±æ€§ä¿¡æ¯ï¼Œå¤šç”¨äºè”è¡¨æŸ¥è¯¢
	* dwd/ods(data warehouse detail)ï¼šäº‹å®æ˜ç»†å±‚ï¼Œå­˜å‚¨äº‹å®è¡¨çš„æ˜ç»†ç²’åº¦æ•°æ®ï¼Œæ¯”è¾ƒåº•å±‚çš„æ•°æ®ï¼Œæºæ•°æ®æ¸…æ´—å¾—æ¥ï¼Œä¾‹å¦‚åŸ‹ç‚¹åæå‡ºæ¥çš„æ•°æ®
	* dwa(data warehouse aggregation)ï¼šäº‹å®èšåˆå±‚ï¼Œå­˜å‚¨äº‹å®è¡¨èšåˆç²’åº¦æ•°æ®ï¼ŒæŒ‰éœ€æ±‚è”åˆæŸ¥è¯¢å¾—åˆ°çš„èšåˆè¡¨
	* app(application)ï¼šåº”ç”¨å±‚ï¼Œå­˜å‚¨ç›´æ¥ä¾›ç»™åº”ç”¨çš„æ•°æ®

	![dwæ¶æ„å›¾](./imgs/dw.png)

* hiveçš„joinæ“ä½œï¼Œåªæ”¯æŒç­‰å€¼åŒ¹é…ï¼Œä¸æ”¯æŒlikeæ¨¡ç³ŠåŒ¹é…ï¼Œå¦‚æœéè¦ä½¿ç”¨likeï¼Œéœ€è¦ä½¿ç”¨ç¬›å¡å°”ç§¯ï¼Œè¿™ä¸ªæ•ˆç‡å¤ªä½ï¼Œä¸å¦‚æ”¾åˆ°å†…å­˜ä¸­åŒ¹é…ï¼Œä¸‹é¢æ˜¯ç¬›å¡å°”ç§¯çš„å†™æ³•

    ```sql
    SELECT table1.brand, SUM(table2.sold) 
    FROM table1, table2
    WHERE table2.product LIKE concat('%', table1.brand, '%') 
    GROUP BY table1.brand;
    ```

* hiveè¡¨åˆ†ä¸ºå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨ï¼Œå†…éƒ¨è¡¨dropçš„æ—¶å€™ä¼šå°†hdfsä¸Šçš„æ•°æ®**ä¸€èµ·åˆ é™¤**ï¼Œå¤–éƒ¨è¡¨dropçš„æ—¶å€™**ä¸ä¼šåˆ é™¤**hdfsä¸Šçš„æ•°æ®

* åˆ›å»ºhiveè¡¨è¯­å¥æ —å­

	```sql
	create external table table_name (
		uid bigint comment 'ç”¨æˆ·id',
		name string
	) comment 'ç”¨æˆ·è¡¨'
	PARTITIONED BY (`date` string)
	ROW FORMAT DELIMITED
		FIELDS TERMINATED BY `\t` // æŒ‡å®šæ¯è¡Œä¸­å­—æ®µåˆ†éš”ç¬¦ä¸º\t
		LINES TERMINATED BY `\n` // æŒ‡å®šè¡Œåˆ†éš”ç¬¦
		COLLECTION ITEMS TERMINATED BY `,` // æŒ‡å®šé›†åˆä¸­å…ƒç´ ä¹‹é—´çš„åˆ†éš”ç¬¦
		MAP KEYS TERMINATED BY `:` // æŒ‡å®šæ•°æ®ä¸­Mapç±»å‹çš„Keyä¸Valueä¹‹é—´çš„åˆ†éš”ç¬¦
	LOCATION
		'hdfs://XXX'
	```
	
	externalæŒ‡ä»£è¿™å¼ è¡¨æ˜¯å¦ä¸ºå¤–éƒ¨è¡¨

* å‘hiveè¡¨ä¸­åŠ è½½æ•°æ®

	* å»ºè¡¨æ—¶ç›´æ¥æŒ‡å®š

		å¦‚æœä½ çš„æ•°æ®å·²ç»åœ¨hdfsä¸Šå­˜åœ¨ï¼Œå·²ç»ä¸ºç»“æ„åŒ–çš„æ•°æ®ï¼Œå¹¶ä¸”æ•°æ®æ‰€åœ¨çš„hdfsè·¯å¾„ä¸éœ€è¦ç»´æŠ¤ï¼Œé‚£ä¹ˆç›´æ¥åœ¨createçš„æ—¶å€™æŒ‡å®šlocationå­—æ®µä¸ºhdfsè·¯å¾„å³å¯
	
	* ä»æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿæˆ–è€…hdfsçš„ä¸€ä¸ªç›®å½•ä¸­åŠ è½½ï¼Œä½¿ç”¨ LOAD DATAå‘½ä»¤åŠ è½½æ•°æ®

		```sql
		load data local inpath XXX overwrite into table partition(day = '20180808') # load æœ¬åœ°æ–‡ä»¶
		
		load data inpath XXX overwrite into table partition(day = '20180808') # load hdfsæ–‡ä»¶
		```
		
	* ä»ä¸€ä¸ªselectæŸ¥è¯¢ä¸­load æ•°æ®

		```sql
		insert overwrite table table_name partition(day = '20180808')
		
		select
			*
		from
			table
		where
			date = '20180808'
		```

* hiveä¸­joinçš„åŸç†å’Œæœºåˆ¶

	ç¬¼ç»Ÿçš„è¯´ï¼Œhiveä¸­çš„joinå¯ä»¥åˆ†ä¸ºcommon join(reduceé˜¶æ®µå®Œæˆjoin)å’Œmap join(mapé˜¶æ®µå®Œæˆjoin)
	
	* mapé˜¶æ®µ
	
		è¯»å–æºè¡¨çš„æ•°æ®ï¼Œmapè¾“å‡ºæ—¶å€™ä»¥join onæ¡ä»¶ä¸­çš„åˆ—ä¸ºkeyï¼Œå¦‚æœJoinæœ‰å¤šä¸ªå…³è”é”®ï¼Œåˆ™ä»¥è¿™äº›å…³è”é”®çš„ç»„åˆä½œä¸ºkeyã€‚mapè¾“å‡ºçš„valueä¸ºjoinä¹‹åæ‰€å…³å¿ƒçš„(selectæˆ–è€…whereä¸­éœ€è¦ç”¨åˆ°çš„)åˆ—ï¼ŒåŒæ—¶åœ¨valueä¸­è¿˜ä¼šåŒ…å«è¡¨çš„Tagä¿¡æ¯ï¼Œç”¨äºæ ‡æ˜æ­¤valueå¯¹åº”å“ªä¸ªè¡¨ï¼›
		
	* shuffleé˜¶æ®µ

		æ ¹æ®keyçš„å€¼è¿›è¡Œhash,å¹¶å°†key/valueæŒ‰ç…§hashå€¼æ¨é€è‡³ä¸åŒçš„reduceä¸­ï¼Œè¿™æ ·ç¡®ä¿ä¸¤ä¸ªè¡¨ä¸­ç›¸åŒçš„keyä½äºåŒä¸€ä¸ªreduceä¸­
		
	* reduceé˜¶æ®µ
		
		æ ¹æ®keyæ•°å€¼å®Œæˆjoinæ“ä½œï¼ŒæœŸé—´é€šè¿‡tagæ¥è¯†åˆ«ä¸åŒè¡¨ä¸­çš„æ•°æ®
		
	* ä¾‹å­

		```sql
		SELECT 
			a.id,
			a.dept,
			b.age 
		FROM
			a join b 
		ON
			a.id = b.id;
		```
		
		![hive-join](./imgs/hive-join.png)

* hive sqlçš„ä¼˜åŒ–

	[Hive SQLçš„ä¼˜åŒ–](http://lxw1234.com/archives/2015/06/317.htm)

* hiveå‡½æ•°æ€»ç»“

    [hiveå‡½æ•°æ€»ç»“](https://www.cnblogs.com/yejibigdata/p/6380744.html)
    
* hiveçš„textå­˜å‚¨æ ¼å¼å’Œparquetå­˜å‚¨æ ¼å¼

	textæ˜¯è¡Œå¼å­˜å‚¨ï¼Œå¤šç”¨äºæ‰‹åŠ¨loadæ•°æ®è¿›å…¥hiveè¡¨ï¼Œä¾‹å¦‚`pandas.Dateframe.tocsv()`
	
	parquetæ˜¯åˆ—å¼å­˜å‚¨ï¼Œåœ¨ä¸€åˆ—æœ‰å¾ˆå¤šç›¸åŒæ•°å€¼(ä¾‹å¦‚NULLå’Œå¸¸æ•°)è¿™æ ·çš„æ—¶å€™ï¼Œç¨€ç–å­˜å‚¨èƒ½çœå¾ˆå¤šç©ºé—´ï¼ŒåŒæ—¶åˆ—å¼å­˜å‚¨åœ¨selectçš„æ—¶å€™ä¸ç”¨éå†æ¯è¡Œï¼Œç›´æ¥éå†åˆ—å°±è¡Œ
	
* hiveä¸­çš„å‹ç¼©è®¾ç½®

	* hive.exec.compress.intermediateï¼šé»˜è®¤è¯¥å€¼ä¸ºfalseï¼Œè®¾ç½®ä¸ºtrueä¸ºæ¿€æ´»ä¸­é—´æ•°æ®å‹ç¼©åŠŸèƒ½ã€‚HiveQLè¯­å¥æœ€ç»ˆä¼šè¢«ç¼–è¯‘æˆHadoopçš„Mapreduce jobï¼Œå¼€å¯Hiveçš„ä¸­é—´æ•°æ®å‹ç¼©åŠŸèƒ½ï¼Œå°±æ˜¯åœ¨MapReduceçš„shuffleé˜¶æ®µå¯¹mapperäº§ç”Ÿçš„ä¸­é—´ç»“æœæ•°æ®å‹ç¼©ã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œä¼˜å…ˆé€‰æ‹©ä¸€ä¸ªä½CPUå¼€é”€çš„ç®—æ³•ã€‚
	* mapred.map.output.compression.codecï¼šè¯¥å‚æ•°æ˜¯å…·ä½“çš„å‹ç¼©ç®—æ³•çš„é…ç½®å‚æ•°ï¼ŒSnappyCodecæ¯”è¾ƒé€‚åˆåœ¨è¿™ç§åœºæ™¯ä¸­ç¼–è§£ç å™¨ï¼Œè¯¥ç®—æ³•ä¼šå¸¦æ¥å¾ˆå¥½çš„å‹ç¼©æ€§èƒ½å’Œè¾ƒä½çš„CPUå¼€é”€ã€‚
	* hive.exec.compress.outputï¼šç”¨æˆ·å¯ä»¥å¯¹æœ€ç»ˆç”Ÿæˆçš„Hiveè¡¨çš„æ•°æ®é€šå¸¸ä¹Ÿéœ€è¦å‹ç¼©ã€‚è¯¥å‚æ•°æ§åˆ¶è¿™ä¸€åŠŸèƒ½çš„æ¿€æ´»ä¸ç¦ç”¨ï¼Œè®¾ç½®ä¸ºtrueæ¥å£°æ˜å°†ç»“æœæ–‡ä»¶è¿›è¡Œå‹ç¼©ã€‚
	* mapred.output.compression.codecï¼šå°†hive.exec.compress.outputå‚æ•°è®¾ç½®æˆtrueåï¼Œç„¶åé€‰æ‹©ä¸€ä¸ªåˆé€‚çš„ç¼–è§£ç å™¨ï¼Œå¦‚é€‰æ‹©SnappyCodecã€‚

		```
		set hive.exec.compress.intermediate=true;
		set mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
		set hive.exec.compress.output=true;
		set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
		```
	
* Hiveä¸­æ–‡ä»¶æ ¼å¼å¯ä»¥åœ¨`create table`çš„æ—¶å€™æŒ‡æ˜ï¼Œé»˜è®¤æ˜¯é‡‡ç”¨textfileçš„æ ¼å¼ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šä¸ºorcï¼Œparquetç­‰

	```
	create table if not exists...
	
	sotred as orc/parquet
	```

* hiveå¯ä»¥é€šè¿‡load local dataå°†æœ¬åœ°æ–‡ä»¶loadåˆ°hdfsä¸Šï¼Œä½†æ˜¯parquetçš„æ–‡ä»¶ä¸èƒ½è¿™æ ·ï¼Œéœ€è¦å…ˆç”¨pandasçš„df.to\_parquet()ï¼Œæ‰å¯ä»¥æ¨ä¸Šå»(è¯¥æ–¹æ³•æ–°å¢äº0.21.0ç‰ˆæœ¬)

	```
	import pandas as pd
	df = pd.DataFrame(data={'col1': [1, 2]})
	df.to_parquet('df.parquet.snappy', compression='snappy')
	pd.read_parquet('df.parquet.snappy')
	...
		col1
	0	1
	1	2
	```

* hiveå‘½ä»¤åé¢çš„é€‰é¡¹

    * hive -fï¼šä½¿ç”¨-fé€‰é¡¹å¯ä»¥è¿è¡ŒæŒ‡å®šæ–‡ä»¶ä¸­çš„å‘½ä»¤ï¼Œ`hive -f script.q`æŒ‡ä»£æˆ‘ä»¬è¿è¡Œè„šæœ¬æ–‡ä»¶`script.q`
    * hive -Sï¼šæ— è®ºæ˜¯åœ¨äº¤äº’å¼è¿˜æ˜¯éäº¤äº’å¼æ¨¡å¼ä¸‹ï¼ŒHiveéƒ½ä¼šæŠŠæ“ä½œè¿è¡Œæ—¶çš„ä¿¡æ¯æ‰“å°è¾“å‡ºåˆ°æ ‡å‡†é”™è¯¯è¾“å‡ºï¼Œä½¿ç”¨-Så¯ä»¥å¼ºåˆ¶ä¸æ˜¾ç¤ºè¿™äº›ä¿¡æ¯
    * hive -eï¼šä½¿ç”¨-eé€‰é¡¹å¯ä»¥åœ¨è¡Œå†…åµŒå…¥å‘½ä»¤ï¼Œä¾‹å¦‚`hive -e 'select * from table'`

* hiveä¸­æŸ¥çœ‹å‡½æ•°ä½¿ç”¨æ–¹æ³•çš„å·¥å…·å‡½æ•°

    `describe function length`æ¥æŸ¥çœ‹lengthçš„ç”¨æ³•

* hiveä»0.14.0ç‰ˆå¼€å§‹å…è®¸ä½¿ç”¨`INSERT INTO TABLE...VALUES`è¯­å¥æ¥æ’å…¥ä¸€å°æ’®ä»¥æ–‡å­—å½¢å¼æŒ‡æ˜çš„è®°å½•ï¼Œå®ƒå¹¶ä¸æ˜¯ç›´æ¥æ’å…¥åˆ°data fileï¼Œè€Œæ˜¯å°†æ•°æ®æ”¾å…¥æš‚å­˜ç›®å½•ï¼Œç”±hiveåº•å±‚çš„åŒæ­¥è¿›ç¨‹å‘¨æœŸæ€§æ‹·è´è¿‡å»

* hiveæ”¯æŒå¤šè¡¨æ’å…¥

    ```
    FROM source
    INSERT OVERWRITE TABLE target1
    select col1
    INSERT OVERWRITE TABLE target2
    select col2;
    ```

* hiveä¸­ä½¿ç”¨order byçš„æ—¶å€™ä¼šå¯¹æ•°æ®è¿›è¡Œå…¨æ’åˆ—ï¼ŒåŒæ—¶åªä¼šä½¿ç”¨ä¸€ä¸ªreducer workerï¼Œæˆ‘ä»¬å¯ä»¥ç”¨sort byå’Œdistribute byæ¥è¿›è¡Œä»£æ›¿ï¼Œå› ä¸ºè¿™ä¸ªæ—¶å€™æˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨è®¾ç½®å¤šä¸ªreducer workerï¼Œæ–¹æ³•å¦‚ä¸‹ï¼š

    ```
    set mapred.reduce.tasks=2;
    ```

* å’Œ Hadoop Streaming ç±»ä¼¼ï¼ŒTRANSFORMã€MAPå’ŒREDUCEå­å¥å¯ä»¥åœ¨Hiveä¸­è°ƒç”¨å¤–éƒ¨è„šæœ¬æˆ–ç¨‹åºï¼Œå¦‚ä¸‹æ‰€ç¤º:

    ```
    ADD FILE /Users/map.py;
    
    select
        transform(year, temperature, quality)
    using
        'python map.py'
    as
        year, temperature
    from
        record
    ```

    ```
    from (
        from record2
        map year, temperature, quality
        using 'python is_good_quality.py'
        as year, temperature)map_output
    reduce year, temperature
    using 'python max_temperature_reduce.py'
    as year, temperature;
    ```

    ä¸Šé¢çš„`map`å’Œ`reduce`å…³é”®å­—éƒ½å¯ä»¥ç”¨`transform`æ¥æ›¿æ¢

* [Hiveä¸­å®ç°Group Byåï¼Œå–Top Kæ¡è®°å½•](https://www.coder4.com/archives/4059)ï¼Œè¿™ç¯‡åšå®¢ç”¨çš„æ˜¯UDFï¼Œpythonçš„è¯ä¹Ÿå¯ä»¥ä½¿ç”¨transformè°ƒç”¨map.pyæ¥å®ç°ç›¸ä¼¼çš„åŠŸèƒ½

* [Hive æ•°æ®å€¾æ–œè§£å†³æ–¹æ¡ˆï¼ˆè°ƒä¼˜ï¼‰](https://blog.csdn.net/s646575997/article/details/51510661)

* hive join ä¼˜åŒ– -- å°è¡¨joinå¤§è¡¨

    * å°ã€å¤§è¡¨join
        
        åœ¨å°è¡¨å’Œå¤§è¡¨è¿›è¡Œjoinçš„æ—¶å€™ï¼Œå°†**å°è¡¨æ”¾åœ¨å‰é¢**ï¼Œæ•ˆç‡ä¼šé«˜ï¼Œhiveä¼šå°†å°è¡¨ç¼“å­˜

    * mapjoin

        ä½¿ç”¨mapjoinå°†å°è¡¨æ”¾å…¥å†…å­˜ï¼Œåœ¨mapç«¯å’Œå¤§è¡¨é€ä¸€åŒ¹é…ï¼Œä»è€Œçœå»reduce

        ```
        select
            /*+mapjoin(b)*/ a.a1,
            a.a2,
            b.b2
        from
            tablea a
        join
            tableb b
        on
            a.a1 = b.b1
        ```
    
        åœ¨0.7ç‰ˆæœ¬åï¼Œä¹Ÿå¯ä»¥ç”¨é…ç½®æ¥è‡ªåŠ¨åŒ–

        ```
        set hive.auto.convert.join=true;
        ```

<h3 id="mapreduce">mapreduce</h3>

* MapReduceç®€ä»‹
    
    MapReduceæ˜¯ä¸€ä¸ªç¼–ç¨‹æ¨¡å‹ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå¤„ç†å’Œç”Ÿæˆè¶…å¤§æ•°æ®é›†çš„ç®—æ³•æ¨¡å‹çš„ç›¸å…³å®ç°ã€‚ç”¨æˆ·é¦–å…ˆåˆ›å»ºä¸€ä¸ªMapå‡½æ•°å¤„ç†ä¸€ä¸ªåŸºäºk/v pairçš„æ•°æ®é›†åˆï¼Œè¾“å‡ºä¸­é—´çš„åŸºäºk/v pairçš„æ•°æ®é›†åˆï¼›ç„¶åå†åˆ›å»ºä¸€ä¸ªReduceå‡½æ•°ç”¨æ¥åˆå¹¶æ‰€æœ‰çš„å…·æœ‰ç›¸åŒä¸­é—´keyå€¼çš„ä¸­é—´valueå€¼ï¼ŒMapReduceæ¶æ„çš„ç¨‹åºèƒ½å¤Ÿåœ¨å¤§é‡çš„æ™®é€šé…ç½®çš„è®¡ç®—æœºä¸Šå®ç°å¹¶è¡ŒåŒ–å¤„ç†ï¼Œå¯ä»¥ç”¨äºå¤„ç†TBçº§åˆ«çš„æ•°æ®

    ![MapReduce](./imgs/mapreduce.png)

	* ç”¨æˆ·ç¨‹åºé¦–å…ˆè°ƒç”¨çš„MapReduceåº“å°†è¾“å…¥æ–‡ä»¶åˆ†æˆMä¸ªæ•°æ®ç‰‡æ®µï¼Œæ¯ä¸ªæ•°æ®ç‰‡æ®µçš„å¤§å°ä»16MBåˆ°512MB(å¯ä»¥é€šè¿‡å¯é€‰çš„å‚æ•°æ¥æ§åˆ¶æ¯ä¸ªæ•°æ®ç‰‡æ®µçš„å¤§å°)ã€‚ç„¶åç”¨æˆ·ç¨‹åºåœ¨æœºç¾¤ä¸­åˆ›å»ºå¤§é‡çš„ç¨‹åºå‰¯æœ¬ã€‚
	* è¿™äº›ç¨‹åºå‰¯æœ¬ä¸­çš„æœ‰ä¸€ä¸ªç‰¹æ®Šçš„ç¨‹åº - masterã€‚å‰¯æœ¬ä¸­å…¶å®ƒçš„ç¨‹åºéƒ½æ˜¯workerç¨‹åºï¼Œç”±masteråˆ†é…ä»»åŠ¡ã€‚æœ‰Mä¸ªMapä»»åŠ¡å’ŒRä¸ªReduceä»»åŠ¡å°†è¢«åˆ†é…ï¼Œmasterå°†ä¸€ä¸ªMapä»»åŠ¡æˆ–Reduceä»»åŠ¡åˆ†é…ç»™ä¸€ä¸ªç©ºé—²çš„workerã€‚
	* è¢«åˆ†é…äº†mapä»»åŠ¡çš„workerç¨‹åºè¯»å–ç›¸å…³çš„è¾“å…¥æ•°æ®ç‰‡æ®µï¼Œä»è¾“å…¥çš„æ•°æ®ç‰‡æ®µä¸­è§£æå‡ºk/v pairï¼Œç„¶åæŠŠk/v pairä¼ é€’ç»™ç”¨æˆ·è‡ªå®šä¹‰çš„Mapå‡½æ•°ï¼Œç”±Mapå‡½æ•°ç”Ÿæˆå¹¶è¾“å‡ºçš„ä¸­é—´k/v pairï¼Œå¹¶ç¼“å­˜åœ¨å†…å­˜ä¸­ã€‚
	* ç¼“å­˜ä¸­çš„k/v pair é€šè¿‡åˆ†åŒºå‡½æ•°åˆ†æˆRä¸ªåŒºåŸŸï¼Œä¹‹åå‘¨æœŸæ€§çš„å†™å…¥åˆ°æœ¬åœ°ç£ç›˜ä¸Šã€‚ç¼“å­˜çš„k/v pairåœ¨æœ¬åœ°ç£ç›˜ä¸Šçš„å­˜å‚¨ä½ç½®å°†è¢«å›ä¼ ç»™masterï¼Œç”±masterè´Ÿè´£æŠŠè¿™äº›å­˜å‚¨ä½ç½®å†ä¼ é€ç»™Reduce workerã€‚
	* å½“Reduce workerç¨‹åºæ¥æ”¶åˆ°masterç¨‹åºå‘æ¥çš„æ•°æ®å­˜å‚¨ä½ç½®ä¿¡æ¯åï¼Œä½¿ç”¨RPCä»Map workeræ‰€åœ¨ä¸»æœºçš„ç£ç›˜ä¸Šè¯»å–è¿™äº›ç¼“å­˜æ•°æ®ã€‚å½“Reduce workerè¯»å–äº†æ‰€æœ‰çš„ä¸­é—´æ•°æ®åï¼Œé€šè¿‡keyè¿›è¡Œæ’åºåä½¿å¾—å…·æœ‰ç›¸åŒkeyå€¼çš„æ•°æ®èšåˆåœ¨ä¸€èµ·ã€‚ç”±äºè®¸å¤šä¸åŒçš„keyå€¼ä¼šæ˜ å°„åˆ°ç›¸åŒçš„Reduceä»»åŠ¡ä¸Šï¼Œå› æ­¤å¿…é¡»æ’åºã€‚å¦‚æœä¸­é—´æ•°æ®å¤ªå¤§æ— æ³•åœ¨å†…å­˜ä¸­å®Œæˆæ’åºï¼Œé‚£ä¹ˆå°±è¦åœ¨å¤–éƒ¨è¿›è¡Œæ’åºã€‚
	* Reduce workerç¨‹åºéå†æ’åºåçš„ä¸­é—´æ•°æ®ï¼Œå¯¹äºæ¯ä¸€ä¸ªå”¯ä¸€çš„ä¸­é—´keyå€¼ï¼ŒReduce workerç¨‹åºå°†è¿™ä¸ªkeyå€¼å’Œå®ƒç›¸å…³çš„ä¸­é—´ value å€¼çš„é›†åˆä¼ é€’ç»™ç”¨æˆ·è‡ªå®šä¹‰çš„ Reduce å‡½æ•°ã€‚Reduce å‡½æ•°çš„è¾“å‡ºè¢«è¿½åŠ åˆ°æ‰€å±åˆ†åŒºçš„è¾“å‡ºæ–‡ä»¶
	* å½“æ‰€æœ‰çš„ Map å’Œ Reduce ä»»åŠ¡éƒ½å®Œæˆä¹‹åï¼Œmaster å”¤é†’ç”¨æˆ·ç¨‹åºã€‚åœ¨è¿™ä¸ªæ—¶å€™ï¼Œåœ¨ç”¨æˆ·ç¨‹åºé‡Œçš„å¯¹ MapReduce è°ƒç”¨æ‰è¿”å›ã€‚

* MapReduceçš„shuffleè¿‡ç¨‹

    * [MapReduce shuffleè¿‡ç¨‹è¯¦è§£](https://blog.csdn.net/u014374284/article/details/49205885) è¿™ç¯‡åšå®¢è®²çš„è¿˜é˜”ä»¥ï¼Œä½†æ˜¯æœ‰ä¸¤ä¸ªåœ°æ–¹æœ‰é—®é¢˜ï¼Œä¸€æ˜¯keyé€šè¿‡hashå–æ¨¡è·å¾—partitionæ˜¯åœ¨è¿›å…¥kvbufferä¹‹åï¼ŒäºŒæ˜¯reduce workerä»map worker copyæ•°æ®ä¸æ˜¯é€šè¿‡httpï¼ŒäºŒæ˜¯é€šè¿‡rpc

    * [Hadoopæ·±å…¥å­¦ä¹ ï¼šMapReduceçš„Shuffleè¿‡ç¨‹è¯¦è§£](http://flyingdutchman.iteye.com/blog/1879642)

    * æ€»çš„æ¥è¯´ï¼Œshuffleé˜¶æ®µå¯ä»¥åˆ†ä¸ºmapç«¯çš„partitioné˜¶æ®µï¼Œsorté˜¶æ®µï¼Œä»¥åŠreduceç«¯çš„copyé˜¶æ®µå’Œmergeé˜¶æ®µ
    
    * reduceç«¯çš„mergeä¸æ˜¯ä¸€æ¬¡æ€§å®Œæˆçš„ï¼Œæ¯”å¦‚ï¼Œå¦‚æœæœ‰50ä¸ªmapè¾“å‡ºï¼Œè€Œåˆå¹¶å› å­æ˜¯10ï¼ˆ10ä¸ºé»˜è®¤å€¼ï¼Œç”±mapreduce.task.io.sort.factorå±æ€§è®¾ç½®ï¼‰ï¼Œåˆå¹¶å°†è¿›è¡Œ5è¶Ÿï¼Œæ¯è¶Ÿå°†10ä¸ªæ–‡ä»¶åˆå¹¶æˆä¸€ä¸ªæ–‡ä»¶ï¼Œå› æ­¤æœ€åæœ‰5ä¸ªä¸­é—´æ–‡ä»¶ï¼Œç„¶åï¼Œå°†è¿™5ä¸ªæ–‡ä»¶ä½œä¸ºreduceçš„è¾“å…¥ï¼Œä»è€Œçœå»äº†ä¸€æ¬¡ç£ç›˜çš„å¾€è¿”è¿‡ç¨‹

        ![reduce-merge](./imgs/reduce-merge.jpg)

* mrçš„inputfileå¯ä»¥å†™å¤šä¸ªï¼Œå¯ä»¥åœ¨map.pyä¸­é€šè¿‡æ•°æ®æ ¼å¼æ¥åŒºåˆ†ä¸åŒçš„æ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡æ¥å¾—åˆ°hdfsä¸Šæ–‡ä»¶çš„ç»å¯¹è·¯å¾„

    [åœ¨mr streamingä¸­è·å–æ–‡ä»¶å](https://blog.csdn.net/bitcarmanlee/article/details/51735053)

* Hadoop Streaming

    ![Hadoop Streaming è®¡ç®—è¿‡ç¨‹](./imgs/hadoop_streaming.jpg)
    
    pythonç¼–å†™mapreduceå°±æ˜¯ä½¿ç”¨äº†Hadoop Streamingçš„ç‰¹ç‚¹
    
    * Streamingçš„ä¼˜ç‚¹ï¼š
    	* å¼€å‘æ•ˆç‡é«˜
    		* åªéœ€æŒ‰ç…§ä¸€å®šçš„æ ¼å¼ä»æ ‡å‡†è¾“å…¥è¯»å–æ•°æ®ã€å‘æ ‡å‡†è¾“å‡ºå†™æ•°æ®å°±è¡Œ
    		* å®¹æ˜“å•æœºè°ƒè¯•: cat input | mapper | sort | reducer > output
    	* ç¨‹åºè¿è¡Œæ•ˆç‡é«˜
			* å¯¹äºCPUå¯†é›†çš„è®¡ç®—ï¼Œæœ‰äº›è¯­è¨€å¦‚C/C++ç¼–å†™çš„ç¨‹åºå¯èƒ½æ¯”ç”¨Javaæ•ˆç‡é«˜ä¸€äº›
		* ä¾¿äºå¹³å°è¿›è¡Œèµ„æºæ§åˆ¶
			* Streamingæ¡†æ¶ä¸­é€šè¿‡limitç­‰æ–¹å¼å¯ä»¥çµæ´»åœ°é™åˆ¶åº”ç”¨ç¨‹åºä½¿ç”¨çš„å†…å­˜èµ„æº
	* Streamingçš„å±€é™
		* Streamingé»˜è®¤åªèƒ½å¤„ç†æ–‡æœ¬æ•°æ®
		* ä¸¤æ¬¡æ•°æ®æ‹·è´å’Œè§£æï¼ˆåˆ†å‰²ï¼‰ï¼Œå¸¦æ¥ä¸€å®šçš„å¼€é”€
	
	* Streamingçš„å¼€å‘è¦ç‚¹ï¼š
		* inputï¼šæŒ‡å®šè¾“å…¥æ–‡ä»¶çš„HDFSè·¯å¾„ï¼Œæ”¯æŒä½¿ç”¨*é€šé…ç¬¦å’ŒæŒ‡å®šå¤šä¸ªæ–‡ä»¶æˆ–ç›®å½•ï¼Œå¯å¤šæ¬¡ä½¿ç”¨
		* outputï¼šæŒ‡å®šè¾“å‡ºæ–‡ä»¶çš„HDFSè·¯å¾„ï¼Œè·¯å¾„å¿…é¡»ä¸å­˜åœ¨ï¼Œä¸”å…·å¤‡åˆ›å»ºè¯¥ç›®å½•çš„æƒé™ï¼Œåªèƒ½ä½¿ç”¨ä¸€æ¬¡
		* mapperï¼šç”¨æˆ·è‡ªå·±å†™çš„mapperç¨‹åº
		* reduerï¼šç”¨æˆ·è‡ªå·±å†™çš„reduceç¨‹åº
		* fileï¼šæ‰“åŒ…æ–‡ä»¶åˆ°æäº¤çš„ä½œä¸šä¸­
			* mapå’Œreduceçš„æ‰§è¡Œæ–‡ä»¶ï¼Œå¦‚run.sh
			* mapå’Œreduceè¦ç”¨è¾“å…¥çš„æ–‡ä»¶ï¼Œå¦‚é…ç½®æ–‡ä»¶
			* è¿˜æœ‰-cacheFile, -cacheArchiveåˆ†åˆ«ç”¨äºå‘è®¡ç®—èŠ‚ç‚¹åˆ†å‘HDFSæ–‡ä»¶å’ŒHDFSå‹ç¼©æ–‡ä»¶
		* jobconfï¼šæäº¤ä½œä¸šçš„ä¸€äº›é…ç½®å±æ€§ï¼Œå¸¸è§é…ç½®ï¼š
			* mapred.map.tasksï¼šmap taskæ•°ç›®
			* mapred.reduce.tasksï¼šreduce taskæ•°ç›®
			* stream.num.map.output.key.fieldsï¼šæŒ‡å®šmap taskè¾“å‡ºè®°å½•ä¸­keyæ‰€å çš„åŸŸæ•°ç›®
			* num.key.fields.for.partitionï¼šæŒ‡å®šå¯¹keyåˆ†å‡ºæ¥çš„å‰å‡ éƒ¨åˆ†åšpartitionï¼Œè€Œéæ•´ä¸ªkey
			* mapred.compress.map.outputï¼šmapçš„è¾“å‡ºæ˜¯å¦å‹ç¼©
			* mapred.map.output.compression.codecï¼šmapçš„è¾“å‡ºå‹ç¼©æ–¹å¼
			* mapred.output.compressï¼šreduceçš„è¾“å‡ºæ˜¯å¦å‹ç¼©
			* mapred.output.compression.codecï¼šreduceçš„è¾“å‡ºå‹ç¼©æ–¹å¼
    		
* mapreduceä¸­çš„combineé˜¶æ®µï¼Œä¼—æ‰€å‘¨çŸ¥ï¼Œmapreduceä¸­æœ‰mapå’Œreduceä¸¤ä¸ªé˜¶æ®µï¼Œå…¶å®è¿˜æœ‰ä¸€ä¸ªç”¨æˆ·å¯ä»¥é€‰æ‹©çš„combineé˜¶æ®µï¼Œå¯¹mapå‡ºæ¥çš„æ•°æ®è¿›è¡Œé¢„èšåˆï¼Œå‡å°‘ä¼ é€’ç»™reduce workerçš„æ•°æ®é‡ï¼ŒåŠ å¿«å¤„ç†é€Ÿåº¦ï¼Œä¾‹å¦‚ï¼Œæ±‚å‡ºæŸä¸ªkeyçš„æœ€å¤§å€¼ï¼Œå°±å¯ä»¥åœ¨map workerä¸­å–å¯¹åº”çš„keyçš„æœ€å¤§å€¼ï¼Œä¸ç”¨å°†æ‰€æœ‰çš„æ•°æ®éƒ½ä¸¢ç»™reduce workerï¼Œcombinerå‡½æ•°åœ¨map æ’åºåçš„è¾“å‡ºä¸Šè¿è¡Œ

* MapReduceæ¡†æ¶åœ¨è®°å½•åˆ°è¾¾reducerä¹‹å‰æŒ‰keyå¯¹è®°å½•æ’åºï¼Œä½†keyæ‰€å¯¹åº”çš„å€¼å¹¶æ²¡æœ‰æ’åºã€‚ç”šè‡³åœ¨ä¸åŒçš„æ‰§è¡Œè½®æ¬¡ä¸­ï¼Œè¿™äº›å€¼çš„æ’åºä¹Ÿä¸å›ºå®šï¼Œå› ä¸ºå®ƒä»¬æ¥è‡ªä¸åŒçš„mapä»»åŠ¡ä¸”è¿™äº›mapä»»åŠ¡åœ¨ä¸åŒè½®æ¬¡ä¸­å®Œæˆæ—¶é—´å„ä¸ç›¸åŒã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¤§å¤šæ•°MapReduceç¨‹åºä¼šé¿å…è®©reduceå‡½æ•°ä¾èµ–äºå€¼çš„æ’åºã€‚ä½†æ˜¯ï¼Œæœ‰æ—¶ä¹Ÿéœ€è¦é€šè¿‡ç‰¹å®šçš„æ–¹æ³•å¯¹keyè¿›è¡Œæ’åºå’Œåˆ†ç»„ç­‰ä»¥å®ç°å¯¹å€¼çš„æ’åºï¼Œä¾‹å¦‚ç»Ÿè®¡æ¯å¹´çš„æœ€é«˜æ°”æ¸©å°±å¾ˆé€‚åˆ

    ```
    hadoop jar path.jar \
        -D stream.num.map.output.key.fields=2 \
        -D mapreduce.partition.keypartitioner.options=-k1,1 \
        -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \
        -D mapreduce.partition.keycomparator.options="-k1n -k2nr" \
        -files map.py,reduce.py
        -input input/all
        -output output
        -mapper "python map.py"
        -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \
        -reducer "python reduce.py"
    ```

    è®¾ç½®`stream.num.map.output.key.fields`ä¸º2ï¼Œç­‰äºè¯´ï¼Œvalueæ˜¯ç©ºï¼Œä½†æ˜¯åœ¨åˆ†åŒºçš„æ—¶å€™ï¼Œåªç”¨keyæ¥åˆ†åŒºï¼Œç¡®ä¿äº†ä¸€è‡´æ€§ï¼Œè®¾ç½®keycomparatorï¼ŒæŒ‰ç…§ç¬¬ä¸€åˆ—å‡åºï¼Œç¬¬äºŒåˆ—é™åºæ¥æ’åºï¼Œå®ç°æ—¢å®šåŠŸèƒ½ï¼Œreduceçš„æ—¶å€™åªéœ€è¦å–å‡ºæ¯ä¸€å¹´çš„ç¬¬ä¸€æ¡è®°å½•å°±è¡Œ

* MapReduceä¸­å¸¸è§çš„joinæ–¹æ³•

    * reduce side join

        reduce side joinæ˜¯ä¸€ç§æœ€ç®€å•çš„joinæ–¹æ³•ï¼Œåœ¨mapé˜¶æ®µåŒæ—¶è¯»å–ä¸¤ä¸ªæ–‡ä»¶file1å’Œfile2ï¼Œä¸ºäº†åŒºåˆ†ä¸¤ç§æ¥æºçš„key/valueæ•°æ®å¯¹ï¼Œç„¶åå¯¹æ¯æ¡æ•°æ®æ‰“ä¸€ä¸ªtagï¼Œæ¯”å¦‚ï¼štag=0è¡¨ç¤ºæ¥è‡ªæ–‡ä»¶File1ï¼Œtag=2è¡¨ç¤ºæ¥è‡ªæ–‡ä»¶File2ã€‚å³ï¼šmapé˜¶æ®µçš„ä¸»è¦ä»»åŠ¡æ˜¯å¯¹ä¸åŒæ–‡ä»¶ä¸­çš„æ•°æ®æ‰“æ ‡ç­¾ã€‚åœ¨reduceé˜¶æ®µï¼Œreduceå‡½æ•°è·å–keyç›¸åŒçš„æ¥è‡ªFile1å’ŒFile2æ–‡ä»¶çš„value listï¼Œ ç„¶åå¯¹äºåŒä¸€ä¸ªkeyï¼Œå¯¹File1å’ŒFile2ä¸­çš„æ•°æ®è¿›è¡Œjoinï¼ˆç¬›å¡å°”ä¹˜ç§¯ï¼‰ã€‚å³ï¼šreduceé˜¶æ®µè¿›è¡Œå®é™…çš„è¿æ¥æ“ä½œ

    * map side join

        ä¹‹æ‰€ä»¥å­˜åœ¨reduce side joinï¼Œæ˜¯å› ä¸ºåœ¨mapé˜¶æ®µä¸èƒ½è·å–æ‰€æœ‰éœ€è¦çš„joinå­—æ®µï¼Œå³ï¼šåŒä¸€ä¸ªkeyå¯¹åº”çš„å­—æ®µå¯èƒ½ä½äºä¸åŒmapä¸­ã€‚Reduce side joinæ˜¯éå¸¸ä½æ•ˆçš„ï¼Œå› ä¸ºshuffleé˜¶æ®µè¦è¿›è¡Œå¤§é‡çš„æ•°æ®ä¼ è¾“ã€‚Map side joinæ˜¯é’ˆå¯¹ä»¥ä¸‹åœºæ™¯è¿›è¡Œçš„ä¼˜åŒ–ï¼šä¸¤ä¸ªå¾…è¿æ¥è¡¨ä¸­ï¼Œæœ‰ä¸€ä¸ªè¡¨éå¸¸å¤§ï¼Œè€Œå¦ä¸€ä¸ªè¡¨éå¸¸å°ï¼Œä»¥è‡³äºå°è¡¨å¯ä»¥ç›´æ¥å­˜æ”¾åˆ°å†…å­˜ä¸­ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å°†å°è¡¨å¤åˆ¶å¤šä»½ï¼Œè®©æ¯ä¸ªmap taskå†…å­˜ä¸­å­˜åœ¨ä¸€ä»½ï¼ˆæ¯”å¦‚å­˜æ”¾åˆ°hash tableä¸­ï¼‰ï¼Œç„¶ååªæ‰«æå¤§è¡¨ï¼šå¯¹äºå¤§è¡¨ä¸­çš„æ¯ä¸€æ¡è®°å½•key/valueï¼Œåœ¨hash tableä¸­æŸ¥æ‰¾æ˜¯å¦æœ‰ç›¸åŒçš„keyçš„è®°å½•ï¼Œå¦‚æœæœ‰ï¼Œåˆ™è¿æ¥åè¾“å‡ºå³å¯

    * SemiJoin

        SemiJoinï¼Œä¹Ÿå«åŠè¿æ¥ï¼Œæ˜¯ä»åˆ†å¸ƒå¼æ•°æ®åº“ä¸­å€Ÿé‰´è¿‡æ¥çš„æ–¹æ³•ã€‚å®ƒçš„äº§ç”ŸåŠ¨æœºæ˜¯ï¼šå¯¹äºreduce side joinï¼Œè·¨æœºå™¨çš„æ•°æ®ä¼ è¾“é‡éå¸¸å¤§ï¼Œè¿™æˆäº†joinæ“ä½œçš„ä¸€ä¸ªç“¶é¢ˆï¼Œå¦‚æœèƒ½å¤Ÿåœ¨mapç«¯è¿‡æ»¤æ‰ä¸ä¼šå‚åŠ joinæ“ä½œçš„æ•°æ®ï¼Œåˆ™å¯ä»¥å¤§å¤§èŠ‚çœç½‘ç»œIOã€‚
å®ç°æ–¹æ³•å¾ˆç®€å•ï¼šé€‰å–ä¸€ä¸ªå°è¡¨ï¼Œå‡è®¾æ˜¯File1ï¼Œå°†å…¶å‚ä¸joinçš„keyæŠ½å–å‡ºæ¥ï¼Œä¿å­˜åˆ°æ–‡ä»¶File3ä¸­ï¼ŒFile3æ–‡ä»¶ä¸€èˆ¬å¾ˆå°ï¼Œå¯ä»¥æ”¾åˆ°å†…å­˜ä¸­ã€‚åœ¨mapé˜¶æ®µï¼Œä½¿ç”¨DistributedCacheå°†File3å¤åˆ¶åˆ°å„ä¸ªTaskTrackerä¸Šï¼Œç„¶åå°†File2ä¸­ä¸åœ¨File3ä¸­çš„keyå¯¹åº”çš„è®°å½•è¿‡æ»¤æ‰ï¼Œå‰©ä¸‹çš„reduceé˜¶æ®µçš„å·¥ä½œä¸reduce side joinç›¸åŒ

    * reduce side join + BloomFilter

        åœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒSemiJoinæŠ½å–å‡ºæ¥çš„å°è¡¨çš„keyé›†åˆåœ¨å†…å­˜ä¸­ä»ç„¶å­˜æ”¾ä¸ä¸‹ï¼Œè¿™æ—¶å€™å¯ä»¥ä½¿ç”¨BloomFilerä»¥èŠ‚çœç©ºé—´ã€‚
BloomFilteræœ€å¸¸è§çš„ä½œç”¨æ˜¯ï¼šåˆ¤æ–­æŸä¸ªå…ƒç´ æ˜¯å¦åœ¨ä¸€ä¸ªé›†åˆé‡Œé¢ã€‚å®ƒæœ€é‡è¦çš„ä¸¤ä¸ªæ–¹æ³•æ˜¯ï¼šadd() å’Œcontains()ã€‚æœ€å¤§çš„ç‰¹ç‚¹æ˜¯ä¸ä¼šå­˜åœ¨false negativeï¼Œå³ï¼šå¦‚æœcontains()è¿”å›falseï¼Œåˆ™è¯¥å…ƒç´ ä¸€å®šä¸åœ¨é›†åˆä¸­ï¼Œä½†ä¼šå­˜åœ¨ä¸€å®šçš„true negativeï¼Œå³ï¼šå¦‚æœcontains()è¿”å›trueï¼Œåˆ™è¯¥å…ƒç´ å¯èƒ½åœ¨é›†åˆä¸­ã€‚å› è€Œå¯å°†å°è¡¨ä¸­çš„keyä¿å­˜åˆ°BloomFilterä¸­ï¼Œåœ¨mapé˜¶æ®µè¿‡æ»¤å¤§è¡¨ï¼Œå¯èƒ½æœ‰ä¸€äº›ä¸åœ¨å°è¡¨ä¸­çš„è®°å½•æ²¡æœ‰è¿‡æ»¤æ‰ï¼ˆä½†æ˜¯åœ¨å°è¡¨ä¸­çš„è®°å½•ä¸€å®šä¸ä¼šè¿‡æ»¤æ‰ï¼‰ï¼Œè¿™æ²¡å…³ç³»ï¼Œåªä¸è¿‡å¢åŠ äº†å°‘é‡çš„ç½‘ç»œIOè€Œå·²

<h3 id="spark">spark</h3>

* spark-clusterçš„å·¥ä½œæ¨¡å¼

    ![spark-cluster](./imgs/spark-cluster.jpg)

* RDDçš„ä¸‰ç§ç”Ÿæˆæ–¹å¼

    * ä»å†…å­˜ä¸­çš„å¯¹è±¡é›†åˆç”Ÿæˆ
    * ä»æœ¬åœ°æ–‡ä»¶æˆ–hdfsä¸­è¯»å–å‡º
    * ä»RDDè½¬æ¢è€Œæ¥

* RDDæ”¯æŒä¸¤ç§ç±»å‹çš„æ“ä½œï¼Œè½¬åŒ–æ“ä½œ(transformation)å’Œè¡ŒåŠ¨æ“ä½œ(action)ï¼Œè½¬åŒ–æ“ä½œä¼šç”±ä¸€ä¸ªRDDç”Ÿæˆä¸€ä¸ªæ–°çš„RDDï¼Œè¡ŒåŠ¨æ“ä½œä¼šå¯¹RDDè®¡ç®—å‡ºä¸€ä¸ªç»“æœï¼Œè½¬åŒ–æ“ä½œå’Œè¡ŒåŠ¨æ“ä½œçš„åŒºåˆ«åœ¨äºSparkè®¡ç®—RDDçš„æ–¹å¼ä¸åŒï¼Œè™½ç„¶ä½ å¯ä»¥åœ¨ä»»ä½•æ—¶å€™å®šä¹‰æ–°çš„RDDï¼Œä½†Sparkåªä¼šæƒ°æ€§è®¡ç®—è¿™äº›RDDï¼Œå®ƒä»¬åªæœ‰ç¬¬ä¸€æ¬¡åœ¨ä¸€ä¸ªè¡ŒåŠ¨æ“ä½œä¸­ç”¨åˆ°æ—¶ï¼Œæ‰ä¼šçœŸæ­£è®¡ç®—

* å¦‚æœåœ¨å¤šä¸ªè¡ŒåŠ¨ä¸­é‡ç”¨åŒä¸€ä¸ªæ“ä½œï¼Œå¯ä»¥ä½¿ç”¨`RDD.persist()`æˆ–`RDD.cache()`è®©SparkæŠŠè¿™ä¸ªRDDç¼“å­˜ä¸‹æ¥ï¼Œæé«˜æ•ˆç‡

* åˆ›å»ºRDDæœ€ç®€å•çš„æ–¹å¼å°±æ˜¯æŠŠç¨‹åºä¸­ä¸€ä¸ªå·²æœ‰çš„é›†åˆä¼ ç»™SparkContextçš„`parallelize()`

    `lines = sc.parallelize(['a', 'b', 'c'])`

* å‘Sparkä¼ é€’å‡½æ•°çš„æ—¶å€™éœ€è¦å°å¿ƒï¼Œpythonä¼šåœ¨ä½ ä¸ç»æ„çš„æ—¶å€™æŠŠå‡½æ•°æ‰€åœ¨çš„å¯¹è±¡ä¹Ÿåºåˆ—åŒ–ä¼ é€’å‡ºå»ï¼Œå½“ä½ ä¼ é€’çš„å¯¹è±¡æ˜¯æŸä¸ªå¯¹è±¡çš„æˆå‘˜ï¼Œæˆ–è€…åŒ…å«äº†å¯¹æŸä¸ªå¯¹è±¡ä¸­ä¸€ä¸ªå­—æ®µçš„å¼•ç”¨æ—¶(ä¾‹å¦‚self.field)ï¼ŒSparkå°±ä¼šæŠŠæ•´ä¸ªå¯¹è±¡å‘é€åˆ°å·¥ä½œèŠ‚ç‚¹ä¸Š

	```python
	class SearchFunctions(object):
		def __init__(self, query):
			self.query = query
		def isMatch(self, s):
			return self.query in s
		def getMatchesFunctionReference(self, rdd):
			# é—®é¢˜: åœ¨"self.isMatch"ä¸­å¼•ç”¨äº†æ•´ä¸ªself
			return rdd.filter(self.isMatch)
		def getMatchesMemberReference(self, rdd):
			# é—®é¢˜: åœ¨"self.query"ä¸­å¼•ç”¨äº†æ•´ä¸ªself
			return rdd.filter(lambda x: self.query in x)
	```
	
	æ›¿ä»£æ–¹æ³•æ˜¯å­˜å‚¨ä¸ºå±€éƒ¨å˜é‡ï¼Œç„¶åä¼ é€’å±€éƒ¨å˜é‡
	
	```python
	class WordFunctions(object):
		...
		def getMatchesMemberReference(self, rdd):
			query = self.query
			return rdd.filter(lambda x: query in x)
	```

* sparkä¸­collectå‡½æ•°å¯ä»¥æ‰“å°å‡ºrddä¸­æ‰€æœ‰çš„æ•°å€¼ï¼Œä½†æ˜¯éœ€è¦ä¿è¯å†…å­˜è£…çš„ä¸‹ï¼ŒcollectAsMapæ–¹æ³•å’Œcollectç±»ä¼¼ï¼Œç”¨äºpair RDDï¼Œæœ€ç»ˆè¿”å›Mapç±»å‹çš„ç»“æœ

    ```python
    rdd = sc.parallelize([(1, 2), (1, 3), (3, 3)])
    rdd.collectAsMap()

    # {1: 3, 3: 3}
    ```

    RDDä¸­åŒä¸€ä¸ªkeyä¸­å­˜æœ‰å¤šä¸ªvalueï¼Œåé¢çš„ä¼šè¦†ç›–å‰é¢çš„ï¼Œæœ€ç»ˆå¾—åˆ°çš„ç»“æœå°±æ˜¯keyå”¯ä¸€

* sparkçš„åˆ†åŒºæ“ä½œ

    sparkèƒ½å¤Ÿå¯¹æ•°æ®é›†åœ¨èŠ‚ç‚¹é—´çš„åˆ†åŒºè¿›è¡Œæ§åˆ¶ï¼Œåœ¨åˆ†å¸ƒå¼ç¨‹åºä¸­ï¼Œé€šä¿¡çš„ä»£ä»·æ˜¯å¾ˆå¤§çš„ï¼Œå› æ­¤æ§åˆ¶æ•°æ®åˆ†å¸ƒä»¥è·å¾—æœ€å°‘çš„ç½‘ç»œä¼ è¾“å¯ä»¥æå¤§åœ°æå‡æ•´ä½“æ€§èƒ½ã€‚åˆ†åŒºå¹¶ä¸æ˜¯å¯¹æ‰€æœ‰çš„åº”ç”¨éƒ½æœ‰å¥½å¤„çš„--æ¯”å¦‚ï¼Œå¦‚æœç»™å®šRDDåªéœ€è¦è¢«æ‰«æä¸€æ¬¡ï¼Œæˆ‘ä»¬å®Œå…¨æ²¡å¿…è¦é¢„å…ˆè¿›è¡Œåˆ†åŒºå¤„ç†ã€‚ç±»ä¼¼`join()`ï¼Œ`cogroup()`ï¼Œ`reduceByKey()`ç­‰æ“ä½œï¼Œåˆ†åŒºå¾ˆæœ‰å¥½å¤„

    pythonä¸­åˆ†åŒºä¾‹å­

    ```python
    rdd.partitionBy(100)
    ```

* Sparkçš„å…±äº«å˜é‡ç±»å‹ï¼šå¹¿æ’­å’Œç´¯åŠ å™¨

    * å¹¿æ’­ï¼Œå¯ä»¥é«˜æ•ˆçš„è®©ç¨‹åºå‘æ‰€æœ‰å·¥ä½œèŠ‚ç‚¹å‘é€ä¸€ä¸ªè¾ƒå¤§çš„åªè¯»å€¼ï¼Œä»¥ä¾›ä¸€ä¸ªæˆ–å¤šä¸ªSparkæ“ä½œä½¿ç”¨

        ```python
        broadcast_var = sc.broadcast(T)

        åœ¨å·¥ä½œèŠ‚ç‚¹å¯ä»¥é€šè¿‡broadcast_var.valueæ¥è·å–å¹¿æ’­å˜é‡
        ```
    
    * ç´¯åŠ å™¨ï¼Œå¯ä»¥åœ¨ä¸åŒçš„å·¥ä½œèŠ‚ç‚¹å†™ç´¯åŠ å™¨ï¼Œç„¶ååœ¨é©±åŠ¨å™¨ç¨‹åºä¸­è°ƒç”¨

        ```python
        # åœ¨Pythonä¸­ç´¯åŠ ç©ºè¡Œ

        file = sc.textFile(inputfile)
        blankLines = sc.accumulator(0)
        
        def extractCallSigns(line):
            global blankLines

            if line == '':
                blankLines += 1

            return line.split(' ')

        callSigns = file.flatMap(extractCallSigns)
        callSigns.saveAsTextFile(outputDir)
        print 'Blank lines: %d' % blankLines.value
        ```

* åŸºäºåˆ†åŒºè¿›è¡Œæ“ä½œ

    * mapPartitions(f)ï¼Œfçš„å‚æ•°æ˜¯å„åˆ†åŒºçš„è¿­ä»£å™¨ï¼Œreturnä¸€ä¸ªè¿­ä»£å™¨

        ```python
        rdd = sc.parallelize([1, 2, 3, 4], 4)
        def f(units): yield sum(units)
        rdd.mapPartitions(f).collect()
        # [3, 7]
        ```

    * mapPartitionsWithIndex(f), fçš„å‚æ•°æ˜¯partitionçš„idxå’Œè¿­ä»£å™¨ï¼Œreturnä¸€ä¸ªè¿­ä»£å™¨

        ```python
        rdd = sc.parallelize([1, 2, 3, 4], 4)
        def f(idx, units): yield idx
        rdd.mapPartitionsWithIndex(f).sum()
        # 6
        ```

    * foreachPartition(f)ï¼Œfçš„å‚æ•°æ˜¯ä¸€ä¸ªè¿­ä»£å™¨

        ```python
        rdd = sc.parallelize([1, 2, 3, 4], 4)
        def f(units):
            for u in units:
                print u
        rdd.foreachPartition(f)
        ```

* sparkåº”ç”¨æäº¤åˆ°é›†ç¾¤ä¸Šçš„æ–¹æ³•: spark-submit --py-files \*.py --master yarn-client python\_file.py

* åœ¨yarn-clientæ¨¡å¼æˆ–è€…ç‹¬ç«‹æ¨¡å¼ä¸‹çš„sparkåº”ç”¨ï¼Œå¯ä»¥åœ¨é©±åŠ¨å™¨ipä¸‹çš„4040ç«¯å£æŸ¥çœ‹sparkä»»åŠ¡çš„çŠ¶æ€ï¼ŒDAGç­‰ä¿¡æ¯ï¼Œå¾ˆæœ‰ç”¨

* sparkæ€§èƒ½ä¼˜åŒ–

    * åˆ©ç”¨åˆ†åŒºæé«˜å¹¶è¡Œåº¦
    * å½“Sparkéœ€è¦é€šè¿‡ç½‘ç»œä¼ è¾“æ•°æ®ï¼Œæˆ–æ˜¯å°†æ•°æ®æº¢å†™åˆ°ç£ç›˜ä¸Šï¼ŒSparkéœ€è¦æŠŠæ•°æ®åºåˆ—åŒ–ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå¯ä»¥é‡‡ç”¨Kryoçš„ç¬¬ä¸‰æ–¹åºåˆ—åŒ–åº“ï¼Œèƒ½å¤Ÿè·å¾—æ›´çŸ­çš„åºåˆ—åŒ–æ—¶é—´å’Œæ›´é«˜çš„å‹ç¼©æ¯”
    * ä½¿ç”¨persistæˆ–è€…cacheæ–¹æ³•ç¼“å­˜åˆ†åŒºï¼Œé¿å…é‡å¤è®¡ç®—
    * è®¾ç½®executorèŠ‚ç‚¹çš„coreså’Œmemory

* spark sqlå¯ä»¥ç›´æ¥é€šè¿‡hivesqlè®¿é—®hiveè¡¨æ ¼çš„æ•°æ®ï¼Œéœ€è¦æŠŠhive\_site.xmlæ”¾åˆ°sparkçš„confæ–‡ä»¶å¤¹ä¸­ï¼Œä¹Ÿå¯ä»¥ç›´æ¥è®¿é—®hdfsçš„parquetï¼Œorcæ–‡ä»¶ï¼Œç„¶åæ³¨å†Œä¸´æ—¶è¡¨

    ```python
    from pyspark.sql import HiveContext

    hiveCtx = HiveContext(sc)
    rows = hiveCtx.sql(hive_sql)

    data = hiveCtx.read.parquet(path of parquet in hdfs)
    data.registerTempTable('table') # ä½œä¸ºä¸´æ—¶è¡¨
    hiveCtx.sql("select * from table")
    ```

* spark sqlå…è®¸ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°(UDF)ï¼Œå¯ä»¥å°†è‡ªå®šä¹‰å‡½æ•°ç±»ä¼¼hiveä¸­çš„countå‡½æ•°ä¸€æ ·ç”¨äºsqlä¸­

    ```python
    hiveCtx.registerFunction('strLen', lambda x: len(x), IntegerType())
    df = hiveCtx.sql("select strLen('name') from table")
    ```

    å¦‚æœè¦è¿”å›ä¸€ä¸ªlistï¼Œå¯ä»¥ä½¿ç”¨typesé‡Œçš„StructFieldå’ŒStructTypeæ¥è‡ªå®šä¹‰

    ```python
    schema = StructType([StructType('name', StringType(), True), StructType('age', IntegerType(), True)])
    udf_func = hiveCtx.registerFunction('udf_func', lambda x: (x, 1), schema)
    ```

* sparkçš„rddå’Œspark.sqlçš„dfçš„æ¨ªè¡Œmergeå’Œçºµå‘mergeæ–¹æ³•

    * rdd
        * æ¨ªå‘ map / mapPartitions
        * çºµå‘ union
    * df
        * æ¨ªå‘ crossJoin(select alias) / join
        * çºµå‘ union

* å’ŒSparkåŸºäºRDDçš„æ¦‚å¿µå¾ˆç›¸ä¼¼ï¼ŒSpark Streamingä½¿ç”¨ç¦»æ•£åŒ–æµä½œä¸ºæŠ½è±¡è¡¨ç¤ºï¼Œå«åšDStreamï¼ŒSpark Streaming ä¼šæŠŠæ¯ä¸ªintervalæ”¶åˆ°çš„æ•°æ®æ”¾å…¥DStream

* Dstreamçš„è½¬åŒ–æ“ä½œå¯ä»¥åˆ†ä¸ºæœ‰çŠ¶æ€å’Œæ— çŠ¶æ€ä¸¤ç§ï¼Œæœ‰çŠ¶æ€çš„å¯ä»¥åˆ›å»ºwindowï¼Œå¤„ç†å¤šä¸ªintervalçš„æ•°æ®

* [Spark Streaming ç®€ä»‹](http://bigdataer.net/?p=244)
    
* [spark å°†dataframeæ•°æ®å†™å…¥hiveè¡¨](https://blog.csdn.net/zgc625238677/article/details/53928320)ï¼ŒåŸºæœ¬æ€è·¯æ˜¯å…ˆå°†dfæ³¨å†Œä¸ºæœ¬åœ°tableï¼Œå†ä»æœ¬åœ°table insertåˆ°hiveè¡¨ä¸­

<h3 id="hbase">hbase</h3>

* hbaseæ˜¯ä¸€ä¸ªåœ¨HDFSä¸Šå¼€å‘çš„é¢å‘åˆ—çš„åˆ†å¸ƒå¼æ•°æ®åº“ï¼Œå¦‚æœéœ€è¦å®æ—¶åœ°éšæœºè®¿é—®è¶…å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå°±å¯ä»¥ä½¿ç”¨HBaseè¿™ä¸€Hadoopåº”ç”¨

* hbaseä¹Ÿæ˜¯ä¸€ä¸ªmaster-slaveçš„å­˜å‚¨æ¨¡å‹ï¼Œå®ƒç”¨ä¸€ä¸ªmasterèŠ‚ç‚¹åè°ƒç®¡ç†ä¸€ä¸ªæˆ–å¤šä¸ªregionserverä»å±æœºã€‚hbaseä¸»æ§æœº(master)è´Ÿè´£å¯åŠ¨ä¸€ä¸ªå…¨æ–°çš„å®‰è£…ï¼ŒæŠŠåŒºåŸŸåˆ†é…ç»™æ³¨å†Œçš„regionserverï¼Œæ¢å¤regionserverçš„æ•…éšœï¼Œmasterçš„è´Ÿè½½å¾ˆè½»ã€‚regionseverè´Ÿè´£é›¶ä¸ªæˆ–å¤šä¸ªçš„åŒºåŸŸç®¡ç†ä»¥åŠå“åº”å®¢æˆ·ç«¯çš„è¯»å†™è¯·æ±‚ã€‚regionserverè¿˜è´Ÿè´£åŒºåŸŸçš„åˆ’åˆ†å¹¶é€šçŸ¥HBase masteræœ‰äº†æ–°çš„å­åŸŸ

    ![hbase-master-slave](./imgs/hbase-master-slave.jpg)

* [HBaseæ·±å…¥æµ…å‡º](https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-bigdata-hbase/index.html)

<h3 id="zk">zookeeper</h3>

* [ZooKeeperç®€ä»‹](https://juejin.im/post/5b970f1c5188255c865e00e7?utm_source=gold_browser_extension)

* ZooKeeperç»´æŠ¤ç€ä¸€ä¸ªæ ‘å½¢å±‚æ¬¡ç»“æ„ï¼Œæ ‘ä¸­çš„èŠ‚ç‚¹è¢«ç§°ä¸ºznodeã€‚znodeå¯ä»¥ç”¨äºå­˜å‚¨æ•°æ®ï¼Œå¹¶ä¸”æœ‰ä¸€ä¸ªä¸ä¹‹ç›¸å…³è”çš„ACL(AccessControlLists)ã€‚ZooKeeperè¢«è®¾è®¡ç”¨æ¥å®ç°åè°ƒæœåŠ¡(è¿™ç±»æœåŠ¡é€šå¸¸ä½¿ç”¨å°æ•°æ®æ–‡ä»¶)ï¼Œè€Œä¸æ˜¯ç”¨äºå¤§å®¹é‡æ•°æ®å­˜å‚¨ï¼Œå› æ­¤ä¸€ä¸ªznodeèƒ½å­˜å‚¨çš„æ•°æ®è¢«é™åˆ¶åœ¨1MBä»¥å†…

* ZooKeeperå¯ä»¥ç”¨æ¥å®ç°åˆ†å¸ƒå¼é”ï¼Œåˆ†å¸ƒå¼é”èƒ½å¤Ÿåœ¨ä¸€ç»„è¿›ç¨‹ä¹‹é—´æä¾›äº’æ–¥æœºåˆ¶ï¼Œä½¿å¾—åœ¨ä»»ä½•æ—¶åˆ»åªæœ‰ä¸€ä¸ªè¿›ç¨‹å¯ä»¥æŒæœ‰é”ã€‚åˆ†å¸ƒå¼é”å¯ä»¥ç”¨äºåœ¨å¤§å‹åˆ†å¸ƒå¼ç³»ç»Ÿä¸­å®ç°é¢†å¯¼è€…é€‰ä¸¾ï¼Œåœ¨ä»»ä½•æ—¶é—´ç‚¹ï¼ŒæŒæœ‰é”çš„é‚£ä¸ªè¿›ç¨‹å°±æ˜¯ç³»ç»Ÿçš„é¢†å¯¼è€…ã€‚ä¸ºäº†ä½¿ç”¨ZooKeeperæ¥å®ç°åˆ†å¸ƒå¼é”æœåŠ¡ï¼Œæˆ‘ä»¬ä½¿ç”¨é¡ºåºznodeæ¥ä¸ºé‚£äº›ç«äº‰é”çš„è¿›ç¨‹å¼ºåˆ¶æ’åºã€‚æ€è·¯å¾ˆç®€å•ï¼šé¦–å…ˆæŒ‡å®šä¸€ä¸ªä½œä¸ºé”çš„znodeï¼Œé€šå¸¸ç”¨å®ƒæ¥æè¿°è¢«é”å®šçš„å®ä½“ï¼Œç§°ä¸º\/leaderï¼Œç„¶åå¸Œæœ›è·å¾—é”çš„å®¢æˆ·ç«¯åˆ›å»ºä¸€äº›çŸ­æš‚é¡ºåºznodeï¼Œä½œä¸ºé”znodeçš„å­èŠ‚ç‚¹ã€‚åœ¨ä»»ä½•æ—¶é—´ç‚¹ï¼Œé¡ºåºå·æœ€å°çš„å®¢æˆ·ç«¯å°†æŒæœ‰é”ã€‚ä¾‹å¦‚ï¼Œæœ‰ä¸¤ä¸ªå®¢æˆ·ç«¯å·®ä¸å¤šåŒæ—¶åˆ›å»ºznodeï¼Œåˆ†åˆ«ä¸º/leader/lock-1å’Œ/leader/lock-2ï¼Œé‚£ä¹ˆåˆ›å»º/leader/lock-1çš„å®¢æˆ·ç«¯å°†ä¼šæŒæœ‰é”ï¼Œå› ä¸ºznodeé¡ºåºå·æœ€å°ï¼Œåªæœ‰å‰ä¸€ä¸ªznodeé‡Šæ”¾äº†é”ï¼Œåä¸€ä¸ªæ‰èƒ½è·å¾—é”

* ä¸ºä»€ä¹ˆæœ€å¥½ä½¿ç”¨å¥‡æ•°å°æœåŠ¡å™¨æ„æˆZooKeeperé›†ç¾¤

    æˆ‘ä»¬çŸ¥é“åœ¨ZooKeeperä¸­Leaderé€‰ä¸¾ç®—æ³•é‡‡ç”¨äº†Zab(ZooKeeper Atomic Broadcast åŸå­å¹¿æ’­)åè®®ã€‚Zabæ ¸å¿ƒæ€æƒ³æ˜¯å½“å¤šæ•°Serverå†™æˆåŠŸï¼Œåˆ™ä»»åŠ¡æ•°æ®å†™æˆåŠŸ

    * å¦‚æœæœ‰3ä¸ªServerï¼Œåˆ™æœ€å¤šå…è®¸1ä¸ªServeræŒ‚æ‰
    * å¦‚æœæœ‰4ä¸ªServerï¼Œåˆ™åŒæ ·æœ€å¤šå…è®¸1ä¸ªServeræŒ‚æ‰
    
    æ—¢ç„¶3ä¸ªæˆ–è€…4ä¸ªServerï¼ŒåŒæ ·æœ€å¤šå…è®¸1ä¸ªServeræŒ‚æ‰ï¼Œé‚£ä¹ˆå®ƒä»¬çš„å¯é æ€§æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥é€‰æ‹©å¥‡æ•°ä¸ªZooKeeper Serverå³å¯

<h3 id="kafka">kafka</h3>

* Kafkaä¸“ä¸ºåˆ†å¸ƒå¼é«˜ååé‡ç³»ç»Ÿè€Œè®¾è®¡ï¼Œæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼å‘å¸ƒ - è®¢é˜…æ¶ˆæ¯ç³»ç»Ÿå’Œä¸€ä¸ªå¼ºå¤§çš„é˜Ÿåˆ—ï¼Œå¯ä»¥å¤„ç†å¤§é‡çš„æ•°æ®ï¼Œå¹¶ä½¿æ‚¨èƒ½å¤Ÿå°†æ¶ˆæ¯ä»ä¸€ä¸ªç«¯ç‚¹ä¼ é€’åˆ°å¦ä¸€ä¸ªç«¯ç‚¹ã€‚ Kafkaé€‚åˆç¦»çº¿å’Œåœ¨çº¿æ¶ˆæ¯æ¶ˆè´¹ã€‚ Kafkaæ¶ˆæ¯ä¿ç•™åœ¨ç£ç›˜ä¸Šï¼Œå¹¶åœ¨ç¾¤é›†å†…å¤åˆ¶ä»¥é˜²æ­¢æ•°æ®ä¸¢å¤±ã€‚ Kafkaæ„å»ºåœ¨ZooKeeperåŒæ­¥æœåŠ¡ä¹‹ä¸Šã€‚ å®ƒä¸Apache Stormå’ŒSparkéå¸¸å¥½åœ°é›†æˆï¼Œç”¨äºå®æ—¶æµå¼æ•°æ®åˆ†æ

* kafkaçš„ä¼˜ç‚¹

    * å¯é æ€§ - Kafkaæ˜¯åˆ†å¸ƒå¼ï¼Œåˆ†åŒºï¼Œå¤åˆ¶å’Œå®¹é”™çš„
    * å¯æ‰©å±•æ€§ - Kafkaæ¶ˆæ¯ä¼ é€’ç³»ç»Ÿè½»æ¾ç¼©æ”¾ï¼Œæ— éœ€åœæœº
    * è€ç”¨æ€§ - Kafkaä½¿ç”¨"åˆ†å¸ƒå¼æäº¤æ—¥å¿—"ï¼Œè¿™æ„å‘³ç€æ¶ˆæ¯ä¼šå°½å¯èƒ½å¿«åœ°ä¿ç•™åœ¨ç£ç›˜ä¸Šï¼Œå› æ­¤å®ƒæ˜¯æŒä¹…çš„
    * æ€§èƒ½ - Kafkaå¯¹äºå‘å¸ƒå’Œè®¢é˜…æ¶ˆæ¯éƒ½æœ‰é«˜ååé‡ã€‚å³ä½¿å­˜å‚¨äº†è®¸å¤šTBçš„æ¶ˆæ¯ï¼Œå®ƒä¹Ÿä¿æŒç¨³å®šçš„æ€§èƒ½ã€‚Kafkaéå¸¸å¿«ï¼Œå¹¶ä¿è¯é›¶åœæœºå’Œé›¶æ•°æ®ä¸¢å¤±

* kafkaæ¶æ„å›¾

    ![kafkaæ¶æ„](./imgs/kafka.jpg)

* kafkaä¸­ä¸€ä¸ªtopicå¯ä»¥ç”±ä¸€ä¸ªconsumer\_groupè®¿é—®ï¼Œgroupä¸­çš„æ¯ä¸ªconsumerè´Ÿè´£ä¸€éƒ¨åˆ†partitionï¼Œå¦‚æœconsumerå’Œkafkaçš„è¿æ¥ç»å¸¸ä¸­æ–­ï¼Œé‚£ä¹ˆä¼šé¢‘ç¹è§¦å‘kafkaçš„rebalanceï¼Œè¿™æ ·å°±ä¼šåœ¨consumerç«¯ç§¯å‹æ•°æ®ï¼Œå¯¼è‡´æ•°æ®æµä¸ä¸‹å»
